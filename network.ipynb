{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as rx\n",
    "import rioxarray as rix\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST FOR PLOTTING\n",
    "def norm_diff(x,y):\n",
    "    return (x-y)/(x+y)\n",
    "\n",
    "child = \"S2A_MSIL2A_20170701T093031_19_10\"\n",
    "\n",
    "def plot_folder(folder,rgb=True):\n",
    "    parent = \"data/\"\n",
    "    if rgb:\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        blue = rix.open_rasterio(parent+f\"{folder}/{folder}_B02.tif\")\n",
    "        green = rix.open_rasterio(parent+f\"{folder}/{folder}_B03.tif\")\n",
    "        red = rix.open_rasterio(parent+f\"{folder}/{folder}_B04.tif\")\n",
    "        print(blue.attrs)\n",
    "        rgb_img = rx.concat([red, green, blue], dim='band')\n",
    "        # rgb_img = (red+green+blue).squeeze()\n",
    "        rgb_img.plot.imshow(robust=True)\n",
    "        plt.title(\"RGB image\")\n",
    "        plt.show()\n",
    "\n",
    "    elif not rgb:\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        nir = rix.open_rasterio(parent+f\"{folder}/{folder}_B8A.tif\")\n",
    "        swir = rix.open_rasterio(parent+f\"{folder}/{folder}_B11.tif\")\n",
    "        avg_ndmi = np.mean(norm_diff(nir.values,swir.values).squeeze())\n",
    "        ndmi_img = norm_diff(nir,swir).squeeze()\n",
    "        print(avg_ndmi)\n",
    "        # plt.imshow(ndmi)\n",
    "        # plt.colorbar(label='Pixel Values')\n",
    "        # plt.title('GeoTIFF Image')\n",
    "        # plt.show()\n",
    "        ndmi_img.plot.imshow(robust=True)\n",
    "        plt.title(f\"NDMI image. Average NDMI:{avg_ndmi:.2f}\")\n",
    "        plt.show()\n",
    "\n",
    "# plot_folder(child,rgb=True)\n",
    "# plot_folder(child,rgb=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S2A_MSIL2A_20170613T101031_0_46' 'S2A_MSIL2A_20170613T101031_0_47'\n",
      " 'S2A_MSIL2A_20170613T101031_0_48' 'S2A_MSIL2A_20170613T101031_0_52'\n",
      " 'S2A_MSIL2A_20170613T101031_0_54' 'S2A_MSIL2A_20170613T101031_0_55'\n",
      " 'S2A_MSIL2A_20170613T101031_0_58' 'S2A_MSIL2A_20170613T101031_0_59'\n",
      " 'S2A_MSIL2A_20170613T101031_0_63' 'S2A_MSIL2A_20170613T101031_0_66'\n",
      " 'S2A_MSIL2A_20170613T101031_0_67' 'S2A_MSIL2A_20170613T101031_0_68'\n",
      " 'S2A_MSIL2A_20170613T101031_0_69' 'S2A_MSIL2A_20170613T101031_0_70'\n",
      " 'S2A_MSIL2A_20170613T101031_0_71' 'S2A_MSIL2A_20170613T101031_0_74'\n",
      " 'S2A_MSIL2A_20170613T101031_0_75' 'S2A_MSIL2A_20170613T101031_0_76'\n",
      " 'S2A_MSIL2A_20170613T101031_0_78' 'S2A_MSIL2A_20170613T101031_0_79'\n",
      " 'S2A_MSIL2A_20170613T101031_0_82' 'S2A_MSIL2A_20170613T101031_0_83'\n",
      " 'S2A_MSIL2A_20170613T101031_0_85' 'S2A_MSIL2A_20170613T101031_0_88'\n",
      " 'S2A_MSIL2A_20170613T101031_0_89' 'S2A_MSIL2A_20170613T101031_1_44'\n",
      " 'S2A_MSIL2A_20170613T101031_1_45' 'S2A_MSIL2A_20170613T101031_1_46'\n",
      " 'S2A_MSIL2A_20170613T101031_1_48' 'S2A_MSIL2A_20170613T101031_1_49'\n",
      " 'S2A_MSIL2A_20170613T101031_1_51' 'S2A_MSIL2A_20170613T101031_1_52'\n",
      " 'S2A_MSIL2A_20170613T101031_1_53' 'S2A_MSIL2A_20170613T101031_1_54'\n",
      " 'S2A_MSIL2A_20170613T101031_1_55' 'S2A_MSIL2A_20170613T101031_1_56'\n",
      " 'S2A_MSIL2A_20170613T101031_1_58' 'S2A_MSIL2A_20170613T101031_1_59'\n",
      " 'S2A_MSIL2A_20170613T101031_1_61' 'S2A_MSIL2A_20170613T101031_7_41'\n",
      " 'S2B_MSIL2A_20170802T092029_15_27' 'S2B_MSIL2A_20170802T092029_15_28'\n",
      " 'S2B_MSIL2A_20170802T092029_15_29' 'S2B_MSIL2A_20170802T092029_15_30'\n",
      " 'S2B_MSIL2A_20170802T092029_15_31' 'S2B_MSIL2A_20170802T092029_15_34'\n",
      " 'S2B_MSIL2A_20170802T092029_15_36' 'S2B_MSIL2A_20170802T092029_15_37'\n",
      " 'S2B_MSIL2A_20170802T092029_15_39' 'S2B_MSIL2A_20170802T092029_15_41']\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def list_folders(directory):\n",
    "    folders = np.array([d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))])\n",
    "    return folders\n",
    "all_folders = list_folders(\"data/\")\n",
    "print(all_folders)\n",
    "print(len(all_folders))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read GeoTIFF file\n",
    "def norm_diff(x,y):\n",
    "    try:\n",
    "        return (x-y)/(x+y)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "# testchild = \"S2A_MSIL2A_20171101T094131_1_38\"\n",
    "\n",
    "\n",
    "def get_input_label(folder):\n",
    "    try:\n",
    "        base_path=f\"data/{folder}/{folder}\"\n",
    "\n",
    "        blue = rix.open_rasterio(f\"{base_path}_B02.tif\")\n",
    "        green = rix.open_rasterio(f\"{base_path}_B03.tif\")\n",
    "        red = rix.open_rasterio(f\"{base_path}_B04.tif\")     \n",
    "\n",
    "        rgb = np.concatenate([red.values,green.values,blue.values], axis=0)\n",
    "\n",
    "        # print(rgb)\n",
    "        nir = rix.open_rasterio(f\"{base_path}_B8A.tif\")\n",
    "        swir = rix.open_rasterio(f\"{base_path}_B11.tif\")\n",
    "\n",
    "        ndmi = norm_diff(nir.values.squeeze(),swir.values.squeeze())\n",
    "        label = np.mean(ndmi)\n",
    "        \n",
    "        return rgb, label\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/S2B_MSIL2A_20170802T092029_15_41/S2B_MSIL2A_20170802T092029_15_41_B02.tif: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "folders = all_folders.copy()\n",
    "\n",
    "input = np.zeros((len(folders), 3, 120, 120))\n",
    "labels = np.zeros((len(folders), 1))\n",
    "\n",
    "for j, folder in enumerate(folders):\n",
    "    rgb, label = get_input_label(folder)\n",
    "    # print(label)\n",
    "    input[j] = rgb\n",
    "    labels[j] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n",
      "50\n",
      "9\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# remove rows from label and input where label is nan or invalid (>1 or <-1)\n",
    "faulty_row = (np.isnan(labels) | (np.abs(labels)>1)).squeeze()\n",
    "print(folders.shape)\n",
    "labels_clean = labels[~faulty_row]\n",
    "input_clean = input[~faulty_row]\n",
    "folders_clean = folders[~faulty_row]\n",
    "# labels = np.clip(labels, -1, 1)\n",
    "print(len(labels))\n",
    "print(len(labels_clean))\n",
    "print(len(input_clean))\n",
    "# print(input[:3])\n",
    "# print(labels[:3])\n",
    "# print(input.shape)\n",
    "np.save(\"arrays/clean_folders.npy\",folders_clean,)\n",
    "np.save(\"arrays/clean_labels.npy\",labels_clean)\n",
    "np.save(\"arrays/clean_input.npy\",input_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert labels to categorical\n",
    "labels_categorical = tf.keras.utils.to_categorical(labels_clean, num_classes=10)\n",
    "\n",
    "# Print the shape of the categorical labels\n",
    "print(labels_categorical.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ChristosTsirogiannis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\ChristosTsirogiannis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# start constructing network for RGB dimension (120,120,3)\n",
    "input_clean = input_clean.reshape(len(input_clean),120,120,3)\n",
    "input_shape = input_clean.shape\n",
    "\n",
    "# Example parameters, replace with your actual values\n",
    "# img_placeholder = tf.placeholder(tf.float32, input_shape)\n",
    "num_classes = 10  # Replace with your actual number of classes\n",
    "is_training = True  # Set to False for inference\n",
    "prediction_threshold = 0.5  # Set the threshold based on your needs\n",
    "\n",
    "\n",
    "input_t = tf.keras.Input(shape=(120,120,3))\n",
    "model_res = tf.keras.applications.resnet50.ResNet50(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_tensor=input_t,\n",
    "    input_shape=(120,120,3),\n",
    "    pooling=None,\n",
    "    classes=10,\n",
    ")\n",
    "# optional for not training the whole model\n",
    "# for layer in model_res.layers[:143]:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# pretrained model\n",
    "model = tf.keras.Sequential()\n",
    "# model.add(tf.keras.layers.Lambda(lambda image: tf.image.resize(image, (224, 224))))\n",
    "model.add(model_res)\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(256, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(64, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.ModelCheckpoint(filepath='beNet50.h5', monitor='val_acc', mode=\"max\", save_best_only=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(input_clean, labels_categorical, epochs=10, batch_size=32, verbose=1, validation_split=0.2, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start constructing network for RGB dimension (120,120,3)\n",
    "input_clean = input_clean.reshape(len(input_clean),120,120,3)\n",
    "input_shape = input_clean.shape\n",
    "\n",
    "# Example parameters, replace with your actual values\n",
    "# img_placeholder = tf.placeholder(tf.float32, input_shape)\n",
    "num_classes = 10  # Replace with your actual number of classes\n",
    "is_training = True  # Set to False for inference\n",
    "prediction_threshold = 0.5  # Set the threshold based on your needs\n",
    "\n",
    "\n",
    "input_t = tf.keras.Input(shape=(120,120,3))\n",
    "model_res = tf.keras.applications.resnet50.ResNet50(\n",
    "    include_top=False,\n",
    "    weights='None',\n",
    "    input_tensor=input_t,\n",
    "    input_shape=(120,120,3),\n",
    "    pooling=None,\n",
    "    classes=10,\n",
    ")\n",
    "# optional for not training the whole model\n",
    "# for layer in model_res.layers[:143]:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# pretrained model\n",
    "model = tf.keras.Sequential()\n",
    "# model.add(tf.keras.layers.Lambda(lambda image: tf.image.resize(image, (224, 224))))\n",
    "model.add(model_res)\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(256, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(64, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urban",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
