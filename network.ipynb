{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ChristosTsirogiannis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as rx\n",
    "import rioxarray as rix\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST FOR PLOTTING\n",
    "def norm_diff(x,y):\n",
    "    return (x-y)/(x+y)\n",
    "\n",
    "child = \"S2A_MSIL2A_20170701T093031_19_10\"\n",
    "\n",
    "def plot_folder(folder,rgb=True):\n",
    "    parent = \"data/\"\n",
    "    if rgb:\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        blue = rix.open_rasterio(parent+f\"{folder}/{folder}_B02.tif\")\n",
    "        green = rix.open_rasterio(parent+f\"{folder}/{folder}_B03.tif\")\n",
    "        red = rix.open_rasterio(parent+f\"{folder}/{folder}_B04.tif\")\n",
    "        print(blue.attrs)\n",
    "        rgb_img = rx.concat([red, green, blue], dim='band')\n",
    "        # rgb_img = (red+green+blue).squeeze()\n",
    "        rgb_img.plot.imshow(robust=True)\n",
    "        plt.title(\"RGB image\")\n",
    "        plt.show()\n",
    "\n",
    "    elif not rgb:\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        nir = rix.open_rasterio(parent+f\"{folder}/{folder}_B8A.tif\")\n",
    "        swir = rix.open_rasterio(parent+f\"{folder}/{folder}_B11.tif\")\n",
    "        avg_ndmi = np.mean(norm_diff(nir.values,swir.values).squeeze())\n",
    "        ndmi_img = norm_diff(nir,swir).squeeze()\n",
    "        print(avg_ndmi)\n",
    "        # plt.imshow(ndmi)\n",
    "        # plt.colorbar(label='Pixel Values')\n",
    "        # plt.title('GeoTIFF Image')\n",
    "        # plt.show()\n",
    "        ndmi_img.plot.imshow(robust=True)\n",
    "        plt.title(f\"NDMI image. Average NDMI:{avg_ndmi:.2f}\")\n",
    "        plt.show()\n",
    "\n",
    "# plot_folder(child,rgb=True)\n",
    "# plot_folder(child,rgb=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S2A_MSIL2A_20170613T101031_0_59' 'S2A_MSIL2A_20170613T101031_0_71']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def list_folders(directory):\n",
    "    folders = np.array([d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))])\n",
    "    return folders\n",
    "all_folders = list_folders(\"data/\")\n",
    "print(all_folders)\n",
    "print(len(all_folders))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read GeoTIFF file\n",
    "def norm_diff(x,y):\n",
    "    try:\n",
    "        return (x-y)/(x+y)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "# testchild = \"S2A_MSIL2A_20171101T094131_1_38\"\n",
    "\n",
    "\n",
    "def get_input_label(folder):\n",
    "    try:\n",
    "        base_path=f\"data/{folder}/{folder}\"\n",
    "\n",
    "        blue = rix.open_rasterio(f\"{base_path}_B02.tif\")\n",
    "        green = rix.open_rasterio(f\"{base_path}_B03.tif\")\n",
    "        red = rix.open_rasterio(f\"{base_path}_B04.tif\")     \n",
    "\n",
    "        rgb = np.concatenate([red.values,green.values,blue.values], axis=0)\n",
    "\n",
    "        # print(rgb)\n",
    "        nir = rix.open_rasterio(f\"{base_path}_B8A.tif\")\n",
    "        swir = rix.open_rasterio(f\"{base_path}_B11.tif\")\n",
    "\n",
    "        ndmi = norm_diff(nir.values.squeeze(),swir.values.squeeze())\n",
    "        label = np.mean(ndmi)\n",
    "        \n",
    "        return rgb, label\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/S2A_MSIL2A_20170613T101031_0_71/S2A_MSIL2A_20170613T101031_0_71_B02.tif: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "folders = all_folders.copy()\n",
    "\n",
    "input = np.zeros((len(folders), 3, 120, 120))\n",
    "labels = np.zeros((len(folders), 1))\n",
    "\n",
    "for j, folder in enumerate(folders):\n",
    "    rgb, label = get_input_label(folder)\n",
    "    # print(label)\n",
    "    input[j] = rgb\n",
    "    labels[j] = label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "2\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# remove rows from label and input where label is nan or invalid (>1 or <-1)\n",
    "faulty_row = (np.isnan(labels) | (np.abs(labels)>1)).squeeze()\n",
    "print(folders.shape)\n",
    "labels_clean = labels[~faulty_row]\n",
    "input_clean = input[~faulty_row]\n",
    "folders_clean = folders[~faulty_row]\n",
    "# labels = np.clip(labels, -1, 1)\n",
    "print(len(labels))\n",
    "print(len(labels_clean))\n",
    "print(len(input_clean))\n",
    "# print(input[:3])\n",
    "# print(labels[:3])\n",
    "# print(input.shape)\n",
    "np.save(\"arrays/clean_folders.npy\",folders_clean,)\n",
    "np.save(\"arrays/clean_labels.npy\",labels_clean)\n",
    "np.save(\"arrays/clean_input.npy\",input_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ChristosTsirogiannis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\ChristosTsirogiannis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 11s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# start constructing network for RGB dimension (120,120,3)\n",
    "input_clean = input_clean.reshape(len(input_clean),120,120,3)\n",
    "input_shape = input_clean.shape\n",
    "\n",
    "# Example parameters, replace with your actual values\n",
    "# img_placeholder = tf.placeholder(tf.float32, input_shape)\n",
    "num_classes = 10  # Replace with your actual number of classes\n",
    "is_training = True  # Set to False for inference\n",
    "prediction_threshold = 0.5  # Set the threshold based on your needs\n",
    "\n",
    "\n",
    "input_t = tf.keras.Input(shape=(120,120,3))\n",
    "model_res = tf.keras.applications.resnet50.ResNet50(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_tensor=input_t,\n",
    "    input_shape=(120,120,3),\n",
    "    pooling=None,\n",
    "    classes=10,\n",
    ")\n",
    "# optional for not training the whole model\n",
    "# for layer in model_res.layers[:143]:\n",
    "#     layer.trainable = False\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Lambda(lambda image: tf.image.resize(image, (224, 224))))\n",
    "model.add(model_res)\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(256, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(64, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ChristosTsirogiannis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Training data contains 0 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.2`. Either provide more data, or a different value for the `validation_split` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ChristosTsirogiannis\\workspace\\university\\urban-computing-project-bigearthnet\\network.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ChristosTsirogiannis/workspace/university/urban-computing-project-bigearthnet/network.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m callback \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mModelCheckpoint(filepath\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbeNet50.h5\u001b[39m\u001b[39m'\u001b[39m, monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_acc\u001b[39m\u001b[39m'\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m\"\u001b[39m, save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ChristosTsirogiannis/workspace/university/urban-computing-project-bigearthnet/network.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ChristosTsirogiannis/workspace/university/urban-computing-project-bigearthnet/network.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(input_clean, labels_clean, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\ChristosTsirogiannis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\ChristosTsirogiannis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1795\u001b[0m, in \u001b[0;36mtrain_validation_split\u001b[1;34m(arrays, validation_split)\u001b[0m\n\u001b[0;32m   1792\u001b[0m split_at \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(math\u001b[39m.\u001b[39mfloor(batch_dim \u001b[39m*\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m validation_split)))\n\u001b[0;32m   1794\u001b[0m \u001b[39mif\u001b[39;00m split_at \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m split_at \u001b[39m==\u001b[39m batch_dim:\n\u001b[1;32m-> 1795\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1796\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTraining data contains \u001b[39m\u001b[39m{batch_dim}\u001b[39;00m\u001b[39m samples, which is not \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1797\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msufficient to split it into a validation and training set as \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1798\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mspecified by `validation_split=\u001b[39m\u001b[39m{validation_split}\u001b[39;00m\u001b[39m`. Either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1799\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mprovide more data, or a different value for the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1800\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`validation_split` argument.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1801\u001b[0m             batch_dim\u001b[39m=\u001b[39mbatch_dim, validation_split\u001b[39m=\u001b[39mvalidation_split\n\u001b[0;32m   1802\u001b[0m         )\n\u001b[0;32m   1803\u001b[0m     )\n\u001b[0;32m   1805\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_split\u001b[39m(t, start, end):\n\u001b[0;32m   1806\u001b[0m     \u001b[39mif\u001b[39;00m t \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Training data contains 0 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.2`. Either provide more data, or a different value for the `validation_split` argument."
     ]
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.ModelCheckpoint(filepath='beNet50.h5', monitor='val_acc', mode=\"max\", save_best_only=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(input_clean, labels_clean, epochs=10, batch_size=32, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Example input data, replace with your actual data\n",
    "    input_data = input_clean\n",
    "\n",
    "    # Run the model\n",
    "    predictions = sess.run(model.predictions, feed_dict={model.img: input_data})\n",
    "    print(\"Predictions:\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urban",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
